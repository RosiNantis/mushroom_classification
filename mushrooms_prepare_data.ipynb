{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from skimage.transform import resize\n",
    "from os import listdir, walk, makedirs, replace\n",
    "from os.path import isfile, join\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make a list of the mushrooms names\n",
    "directory = 'clean_data/'\n",
    "folder_list= [f for f in listdir(directory)]\n",
    "# folder_list.remove('mushroom_sample')\n",
    "# folder_list.remove('mushroom_sample.zip')\n",
    "folder_list.remove('.DS_Store')\n",
    "folder_list.remove('data_pool')\n",
    "folder_list\n",
    "CLASSES = dict(zip(folder_list, range(len(folder_list)))) # dictionary of the mushrooms with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: call its folder category separately and make an array of items from the images\n",
    "def get_label(key ,class_mush = CLASSES):\n",
    "    \"\"\"\n",
    "    You can get the labeled name of the mushroom by giving \n",
    "    the index or the index by giving the key\n",
    "    \"\"\"\n",
    "    if type(key) == str:\n",
    "        return class_mush[key]\n",
    "    elif type(key) == int:\n",
    "        return list(CLASSES.keys())[list(CLASSES.values()).index(key)]\n",
    "\n",
    "\n",
    "def get_images(fl ):\n",
    "    \"\"\"\n",
    "    give the list of photos from directory and the labels.\n",
    "    fl = folder of images\n",
    "    \"\"\"\n",
    "    directory = 'data/Data/Mushrooms/'\n",
    "    label = []\n",
    "    mush= []\n",
    "    pictures_dir = directory + fl + '/'\n",
    "    pictures = [pics for pics in listdir(pictures_dir) if isfile(join(pictures_dir , pics))]\n",
    "    for i in pictures:\n",
    "        mush.append(cv2.imread(pictures_dir+i))\n",
    "        label.append(get_label(fl))     \n",
    "    return mush, label, len(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO split data train valid test (75% train, 15% valid, 10% test)\n",
    "def split_preprocessed_data(X  ,y , test_size = 0.1):\n",
    "    \"\"\"\n",
    "    This function returns the train and validataion data \n",
    "    preprocessed if necessary: in a flattened 1_D form\n",
    "    with normalized X and y per 255.\n",
    "    Split images (75%/15%/10%) and save to temporary folders\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state=42)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = test_size * 1.7, random_state=42)  \n",
    "    where ='clean_data/data_pool/'+ get_label(y[0]) + '/'\n",
    "    with open('X_train','wb') as f: pickle.dump(X_train, f)\n",
    "    os.makedirs(where + 'train/', exist_ok=True)\n",
    "    os.replace('X_train', where+ 'train/X_train')\n",
    "    with open('y_train','wb') as f: pickle.dump(y_train, f)\n",
    "    os.replace('y_train', where+ 'train/y_train')\n",
    "    os.makedirs(where + 'test/', exist_ok=True)\n",
    "    with open('X_test','wb') as f: pickle.dump(X_test, f)\n",
    "    os.replace('X_test', where+ 'test/X_test')\n",
    "    with open('y_test','wb') as f: pickle.dump(y_test, f)\n",
    "    os.replace('y_test', where+ 'test/y_test')\n",
    "    os.makedirs(where + 'valid/', exist_ok=True)\n",
    "    with open('X_valid','wb') as f: pickle.dump(X_valid, f)\n",
    "    os.replace('X_valid', where+ 'valid/X_valid')\n",
    "    with open('y_valid','wb') as f: pickle.dump(y_valid, f)\n",
    "    os.replace('y_valid', where+ 'valid/y_valid')   \n",
    "    return X_train, X_test, y_train, y_test, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_images(image, number_of_extra, save_to_dir, save_prefix):\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True)\n",
    "    image = image\n",
    "    l = image.reshape((1,) + image.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "\n",
    "    # the .flow() command below generates batches of randomly transformed images\n",
    "    # and saves the results to the `preview/` directory\n",
    "    i = 0\n",
    "\n",
    "    for batch in datagen.flow(l, batch_size=1,\n",
    "            save_to_dir=save_to_dir, save_prefix=save_prefix, save_format='jpg'):     \n",
    "\n",
    "            i += 1\n",
    "            if i > number_of_extra-1:\n",
    "                    break  # otherwise the generator would loop indefinitel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add augmented images in the X_train y_train arrays\n",
    "def get_augmented_images(fl, directory='clean_data/data_pool/'):\n",
    "    \"\"\"\n",
    "    give the list of photos from directory and the labels.\n",
    "    fl = folder of images with naming the mashroom\n",
    "    \"\"\"\n",
    "    directory = directory #'data/Data/Mushrooms/'\n",
    "\n",
    "    label = []\n",
    "    mush= []\n",
    "    pictures_dir = directory + fl + '/test/'\n",
    "    pictures = [pics for pics in listdir(pictures_dir)]# if isfile(join(pictures_dir , pics))]\n",
    "    for i in pictures:\n",
    "        mush.append(cv2.imread(pictures_dir+i))\n",
    "        label.append(get_label(fl))     \n",
    "    return mush, label, len(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_save(X_train=X_train,mush=mush,y_train=y_train,label=label):\n",
    "    X_train = np.concatenate((X_train,mush))\n",
    "    y_train = np.concatenate((y_train,label))\n",
    "\n",
    "    where ='clean_data/data_pool/'+ get_label(label[0]) + '/test_full/'\n",
    "    os.makedirs(where , exist_ok=True)\n",
    "    with open('X_train','wb') as f: pickle.dump(X_train, f)\n",
    "    os.replace('X_train', where + 'X_train')\n",
    "    with open('y_train','wb') as f: pickle.dump(y_train, f)\n",
    "    os.replace('y_train', where + 'y_train')\n",
    "    return X_train, y_train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the process for all mushrooms\n",
    "for f in folder_list:\n",
    "    mush, label, length = get_images(f)\n",
    "    X_train, X_test, y_train, y_test, X_valid, y_valid = split_preprocessed_data(mush,label)\n",
    "    # TODO: augmentation for the mushrooms with small number of images in train and valid\n",
    "    if len(y_train)/750 < 0.8:\n",
    "        for i in range(len(y_train)):\n",
    "            augmented_images(X_train[i] ,2 , 'clean_data/data_pool/'+f+'/train','train')\n",
    "\n",
    "    mush, label ,length =get_augmented_images(f)\n",
    "    X_train, y_train = concat_save()\n",
    "\n",
    "# took 22 minutes to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image reshape per folder\n",
    "\n",
    "def image_reshape_train(image, new_size = (224,224)):\n",
    "    \"\"\"\n",
    "    give the list of photos with the correct pixel size you want to \n",
    "    downsize. \n",
    "    image is the folder with the name of mushroom as string, i.e. 'Agaricus'\n",
    "    \"\"\"\n",
    "    directory = 'clean_data/data_pool/'+ image + '/'\n",
    "    if os.path.exists(directory + 'train_full/'):\n",
    "        directory = directory + 'train_full/'\n",
    "    elif os.path.exists(directory + 'train/'):\n",
    "        directory = directory + 'train/'\n",
    "\n",
    "    with open(directory + 'X_train', \"rb\") as fp:  X_train = pickle.load(fp)\n",
    "    with open(directory + 'y_train', \"rb\") as fp:  y_train = pickle.load(fp)\n",
    "    image = X_train\n",
    "    label = y_train\n",
    "    items_resized = []\n",
    "    for idx, dir in enumerate(image):     \n",
    "        item_resized_colored = resize(dir, new_size)\n",
    "        items_resized.append(item_resized_colored)\n",
    "    directory = 'clean_data/data_pool/'+ get_label(label[0]) + '/'\n",
    "    with open(directory + \"X_train_resized\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(items_resized, fp)\n",
    "    \n",
    "# for folder in folder_list:\n",
    "#     image_reshape_train(folder, )\n",
    "    \n",
    "def image_reshape(image, new_size = (224,224)):\n",
    "    \"\"\"\n",
    "    give the list of photos with the correct pixel size you want to \n",
    "    downsize. \n",
    "    image is the folder with the name of mushroom as string, i.e. 'Agaricus'\n",
    "    \"\"\"\n",
    "    # for the test\n",
    "    # directory = 'clean_data/data_pool/'+ image + '/test/'\n",
    "    # with open(directory + 'X_test', \"rb\") as fp:  X_test = pickle.load(fp)\n",
    "    # with open(directory + 'y_test', \"rb\") as fp:  y_test = pickle.load(fp)\n",
    "    # image = X_test\n",
    "    # label = y_test\n",
    "    # items_resized = []\n",
    "    # for idx, dir in enumerate(image):     \n",
    "    #     item_resized_colored = resize(dir, new_size)\n",
    "    #     items_resized.append(item_resized_colored)\n",
    "    # directory = 'clean_data/data_pool/'+ get_label(label[0]) + '/'\n",
    "    # with open(directory + \"X_test_resized\", \"wb\") as fp:\n",
    "    #     pickle.dump(items_resized, fp)\n",
    "    #     # for the valid\n",
    "    directory = 'clean_data/data_pool/'+ image + '/valid/'\n",
    "    with open(directory + 'X_valid', \"rb\") as fp:  X_valid = pickle.load(fp)\n",
    "    with open(directory + 'y_valid', \"rb\") as fp:  y_valid = pickle.load(fp)\n",
    "    image = X_valid\n",
    "    label = y_valid\n",
    "    items_resized = []\n",
    "    for idx, dir in enumerate(image):     \n",
    "        item_resized_colored = resize(dir, new_size)\n",
    "        items_resized.append(item_resized_colored)\n",
    "    directory = 'clean_data/data_pool/'+ get_label(label[0]) + '/'\n",
    "    with open(directory + \"X_valid_resized\", \"wb\") as fp:\n",
    "        pickle.dump(items_resized, fp)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in folder_list:\n",
    "    image_reshape_train(f)\n",
    "\n",
    "# 10 minutes to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in folder_list:\n",
    "\n",
    "    image_reshape(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: unpickle and connect\n",
    "def unpickle(directory, category ):\n",
    "    \"\"\"\n",
    "    This functions takes the data from a directory that have been\n",
    "    downsized in banches ang gives back the X,y as arrays.\n",
    "    \"\"\"\n",
    "    table_mushrooms_downsized = []\n",
    "    label_mushrooms = []\n",
    "    for mushroom in CLASSES:\n",
    "\n",
    "        dir = directory + mushroom + '/X_' + category + '_resized'\n",
    "        with open(dir, \"rb\") as fp:  X_resized = pickle.load(fp)\n",
    "        table_mushrooms_downsized.append(X_resized)\n",
    "        label = np.full((1, len(X_resized)), get_label(mushroom))#.ravel()\n",
    "        label_mushrooms = np.append(label_mushrooms,label)\n",
    "\n",
    "    # flatten the nested list into a 1D list\n",
    "    mushrooms_downsized = list(itertools.chain(*table_mushrooms_downsized))\n",
    "    return mushrooms_downsized, label_mushrooms\n",
    "\n",
    "    # if len(mushrooms_downsized) == len(label):\n",
    "\n",
    "    # #     #Check if the number of pictures and labels are the same.\n",
    "    #     X = np.asarray(mushrooms_downsized)\n",
    "    #     y = np.asarray(label_mushrooms)\n",
    "\n",
    "    #return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ca in [\"train\",\"valid\"]:\n",
    "    category = ca\n",
    "    X, y = unpickle(directory=\"clean_data/data_pool/\", category = category)\n",
    "    directory = 'clean_data/data_pool/'\n",
    "    # shuffle the samples and labels together. Since it \n",
    "    # shuffler = np.random.permutation(len(X))\n",
    "    # X = np.array(X)\n",
    "    # y = np.array(y)\n",
    "    # X = X[shuffler]\n",
    "    # y = y[shuffler]\n",
    "    with open(directory + \"X\" + category, \"wb\") as fp: pickle.dump(X, fp)\n",
    "    with open(directory + \"y\" + category, \"wb\") as fp: pickle.dump(y, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the samples and labels together\n",
    "shuffler = np.random.permutation(len(X))\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X = X[shuffler]\n",
    "y = y[shuffler]\n",
    "with open(directory + \"X\" + category, \"wb\") as fp: pickle.dump(X, fp)\n",
    "with open(directory + \"y\" + category, \"wb\") as fp: pickle.dump(y, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[1000],cmap='Greys')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir ='clean_data/data_pool/Xtrain'\n",
    "with open(dir, \"rb\") as fp:  X_train = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/08/6zrs8w253573k0k0yjq1w49c0000gn/T/ipykernel_9680/509496145.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: unpickle and connect\n",
    "def unpickle(directory, category ):\n",
    "    \"\"\"\n",
    "    This functions takes the data from a directory that have been\n",
    "    downsized in banches ang gives back the X,y as arrays.\n",
    "    \"\"\"\n",
    "    label_mushrooms = []\n",
    "    for mushroom in CLASSES:\n",
    "\n",
    "        dir = directory + mushroom + '/X_' + category + '_resized'\n",
    "        with open(dir, \"rb\") as fp:  X_resized = pickle.load(fp)\n",
    "        label = np.full((1, len(X_resized)), get_label(mushroom))#.ravel()\n",
    "        label_mushrooms = np.append(label_mushrooms,label)\n",
    "\n",
    "    return  label_mushrooms\n",
    "\n",
    "    # if len(mushrooms_downsized) == len(label):\n",
    "\n",
    "    # #     #Check if the number of pictures and labels are the same.\n",
    "    #     X = np.asarray(mushrooms_downsized)\n",
    "    #     y = np.asarray(label_mushrooms)\n",
    "\n",
    "    #return X, y\n",
    "for ca in [\"train\"]:\n",
    "    category = ca\n",
    "    y = unpickle(directory=\"clean_data/data_pool/\", category = category)\n",
    "    directory = 'clean_data/data_pool/'\n",
    "\n",
    "    with open(directory + \"y\" + category, \"wb\") as fp: pickle.dump(y, fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/08/6zrs8w253573k0k0yjq1w49c0000gn/T/ipykernel_9680/193313361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'clean_data/data_pool/Xtest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'clean_data/data_pool/ytest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "with open('clean_data/data_pool/Xtest', \"rb\") as fp:  X = pickle.load(fp)\n",
    "with open('clean_data/data_pool/ytest', \"rb\") as fp:  y = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2, decode_predictions, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image # Keras own inbuild image class\n",
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras\n",
    "\n",
    "def model_ResNet50V2(CLASSES = CLASSES,learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    This function returns a ResNet50V2 model with the last\n",
    "    layer removed.\n",
    "    \"\"\"\n",
    "    K.clear_session() # Always clear the session!\n",
    "\n",
    "    base_model = ResNet50V2(\n",
    "        weights='imagenet', \n",
    "        pooling='avg',      # applies global average pooling to the output of the last conv layer (like a flattening)\n",
    "        include_top=False,   # !!!!! we only want to have the base, not the final dense layers \n",
    "        input_shape=(224, 224, 3)  \n",
    "    )\n",
    "    base_model.trainable = False # To freeze the model\n",
    "    # Start building on top of the model\n",
    "    model = keras.Sequential() # defining a new model\n",
    "    model.add(base_model) # adding in the pretrained model without the fully connected layer\n",
    "    model.add(keras.layers.Dense(64, activation='relu')) # adding in additional layers\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(128, activation='relu')) # adding in additional layers\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(len(CLASSES), activation='softmax')) #!!! Final layer with a length of 2, and softmax activation \n",
    "    # have a look at the trainable and non-trainable params statistic\n",
    "    model.summary()\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate),\n",
    "              loss=keras.losses.categorical_crossentropy,\n",
    "              metrics=[keras.metrics.categorical_accuracy])\n",
    "\n",
    "    return model\n",
    "\n",
    "def fit_ResNet50V2(X_train, y_train,epochs = 200, batch_size =500,sav = True):\n",
    "    \"\"\"\n",
    "    This function fits the model on the training data.\n",
    "    \"\"\"\n",
    "    # observe the validation loss and stop when it does not improve after 3 iterations\n",
    "    callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "    history = model.fit(X_train, y_train, \n",
    "              epochs=epochs, \n",
    "              verbose=2,\n",
    "              batch_size=batch_size, \n",
    "              callbacks=[callback],\n",
    "              # use 30% of the data for validation\n",
    "              validation_split=0.3)\n",
    "    # save model\n",
    "    if sav == True:\n",
    "        model.save('models/ResNet50V2_RGB_300fot_SD.h5')\n",
    "        \n",
    "    return history\n",
    "model = model_ResNet50V2()\n",
    "\n",
    "history = fit_ResNet50V2(X, y, epochs = 200, batch_size =300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
