{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from skimage.transform import resize\n",
    "from os import listdir, walk, makedirs, replace\n",
    "from os.path import isfile, join\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test0 = pd.read_csv('data/Data/fungiclef2022/FungiCLEF2022_test_metadata.csv',sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test0.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START OF DATA MIX BETWEEN BIG AND SMALL DATA SET\n",
    "- ATTENTION the lines below do not work systimatically but have worke once and now I keep them as a \n",
    "reminder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Gives a list of all the files in the directory with the highest photographed mashroom.\n",
    "# TODO: make folders of the top20 most frequent mushrooms and put them in data folders.\n",
    "\n",
    "train_BigData = pd.read_csv('data/Data/fungiclef2022/DF20-train_metadata.csv',sep = ',')\n",
    "# find the 20 categories with the most pictures stored\n",
    "train_BigData['counts'] = 1\n",
    "train_BigData.groupby(['scientificName']).sum().nlargest(20, 'counts')\n",
    "new_fungi = train_BigData.groupby(['scientificName']).sum().nlargest(20, 'counts')\n",
    "\n",
    "fungi = new_fungi[[\"counts\"]]\n",
    "fungi = fungi.reset_index(level=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=20\n",
    "G = train_BigData.loc[train_BigData['scientificName'] == fungi.scientificName[k]]\n",
    "print(fungi.scientificName[k])\n",
    "g = G.image_path.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make directory\n",
    "name ='Meripilus_giganteus'\n",
    "os.makedirs( 'data/Data/Mushrooms/'+name)\n",
    "\n",
    "directory = 'data/Data/fungiclef2022/DF20-300px_train/DF20_300/' \n",
    "image_list = []\n",
    "for ls in G.image_path.tolist():\n",
    "    image_list.append(directory + ls)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move images. Mix BIG with SMALL KAggle data set to result in a total of (29 species)\n",
    "\n",
    "def move_image(name):\n",
    "    \"\"\"\n",
    "    Read files from directory\n",
    "    \"\"\"\n",
    "    directory = 'data/Data/Mushrooms/'+name+'/' \n",
    "\n",
    "    for idx, ls in enumerate(image_list):\n",
    "        # print(ls)\n",
    "        # print(directory+ g[idx])\n",
    "        os.replace(ls, directory+ g[idx])\n",
    "\n",
    "move_image(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF DATA MIX BETWEEN BIG AND SMALL DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO split data train valid test (75% train, 15% valid, 10% test)\n",
    "\n",
    "# TODO: augmentation for the mushrooms with small number of images in train and valid\n",
    "\n",
    "# TODO: downsize each image per folder and pickle a banch per kind of mushroom\n",
    "\n",
    "# TODO: unpickle and connect\n",
    "\n",
    "# TODO: model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- During night we attempt to downsize the images from Big Data from the train data while keeping the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'data/Data/fungiclef2022/DF20-300px_train/DF20_300/' \n",
    "train_BigData['full_path_image'] = directory + train_BigData.image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of labels and directories\n",
    "BIG_DATA_train = train_BigData.filter(['scientificName','image_path', 'full_path_image'], axis=1)\n",
    "#downsize_in_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_DATA_train.full_path_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_DATA_train.scientificName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_reshape_BD(dir, batch, new_size = (224,224),k = 300, gray =False):\n",
    "    \"\"\"\n",
    "    works for Data from csv and random labels.\n",
    "    give the list of photos with the correct pixel size you want to \n",
    "    downsize. If you want Gray then write True.\n",
    "    \"\"\"\n",
    "    items_resized = []\n",
    "    label = []\n",
    "    for idx, dir in enumerate(dir[(batch-1)*k:batch * k],(batch-1)*k):     \n",
    "        item = cv2.imread(dir)\n",
    "        label.append(idx)\n",
    "        if gray == True:\n",
    "            item_gray = cv2.cvtColor(item, cv2.COLOR_BGR2GRAY)\n",
    "            item_resized_gray = resize(item_gray, new_size)\n",
    "            items_resized.append(item_resized_gray)\n",
    "        else:\n",
    "            item_resized_colored = resize(item, new_size)\n",
    "            items_resized.append(item_resized_colored)\n",
    "    direct = 'data/Data/fungiclef2022/DF20-300px_train/' \n",
    "    with open(direct + \"downsized/mushrooms_resized_part_\"+ str(batch), \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(items_resized, fp)\n",
    "    return items_resized, label\n",
    "    \n",
    "def downsize_in_batches(directories):\n",
    "    \"\"\"\n",
    "    This function downsizes the images in batches of 300pics.\n",
    "    directories = list of the directories of the images.\n",
    "    \"\"\"\n",
    "    batch = 1\n",
    "    for idx, dir in enumerate(directories):\n",
    "        if idx % 300 == 0:\n",
    "            print(batch)\n",
    "            image_reshape_BD(directories,batch)\n",
    "            batch += 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of labels and directories\n",
    "downsize_in_batches(BIG_DATA_train.full_path_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- During night we attempt to downsize the images from Big Data from the valid data while keeping the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_BigData = pd.read_csv('data/Data/fungiclef2022/FungiCLEF2022_test_metadata.csv',sep = ',')\n",
    "test_BigData.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'data/Data/fungiclef2022/DF21-images-300_val/DF21_300/' \n",
    "test_BigData['full_path_image'] = directory + test_BigData.filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of labels and directories\n",
    "BIG_DATA_test = test_BigData.filter(['ObservationId','filename', 'full_path_image'], axis=1)\n",
    "#downsize_in_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_reshape_BD(dir, batch, new_size = (224,224),k = 300, gray =False):\n",
    "    \"\"\"\n",
    "    works for Data from csv and random labels.\n",
    "    give the list of photos with the correct pixel size you want to \n",
    "    downsize. If you want Gray then write True.\n",
    "    \"\"\"\n",
    "    items_resized = []\n",
    "    label = []\n",
    "    for idx, dir in enumerate(dir[(batch-1)*k:batch * k],(batch-1)*k):     \n",
    "        item = cv2.imread(dir)\n",
    "        label.append(idx)\n",
    "        if gray == True:\n",
    "            item_gray = cv2.cvtColor(item, cv2.COLOR_BGR2GRAY)\n",
    "            item_resized_gray = resize(item_gray, new_size)\n",
    "            items_resized.append(item_resized_gray)\n",
    "        else:\n",
    "            item_resized_colored = resize(item, new_size)\n",
    "            items_resized.append(item_resized_colored)\n",
    "    direct = 'data/Data/fungiclef2022/DF21-images-300_val/' \n",
    "    with open(direct + \"downsized/mushrooms_resized_part_\"+ str(batch), \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(items_resized, fp)\n",
    "    return items_resized, label\n",
    "    \n",
    "def downsize_in_batches(directories):\n",
    "    \"\"\"\n",
    "    This function downsizes the images in batches of 300pics.\n",
    "    directories = list of the directories of the images.\n",
    "    \"\"\"\n",
    "    batch = 1\n",
    "    for idx, dir in enumerate(directories):\n",
    "        if idx % 300 == 0:\n",
    "            print(batch)\n",
    "            image_reshape_BD(directories,batch)\n",
    "            batch += 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of labels and directories\n",
    "downsize_in_batches(BIG_DATA_test.full_path_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From Big Data mushrooms we have two lists, splitted in test and validation.\n",
    "- - 1604 kinds of fungis\n",
    "- - 266344 / 19594 , test/ val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Smalll data set\n",
    "- - Categories : 'Agaricus' : 0,'Amanita' : 1,'Boletus' : 2,'Cortinarius' : 3,'Entoloma' : 4,'Hygrocybe' : 5,\n",
    "    'Lactarius' : 6,'Russula' : 7,'Suillus' : 8\n",
    "- - number of pics per category: [352, 749, 1072, 835, 363, 315, 1562, 1147, 310]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day-1\n",
    "- Work on the small data set.\n",
    "- - make batches of downsize files\n",
    "- - see issue with the premature end of JPEG\n",
    "- - - make model run with 300 img per category\n",
    "- - - make model work for 1000 pics per category using augmentation.Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getList(dict):\n",
    "    \"\"\"\n",
    "    This function returns the list of keys in a dictionary\n",
    "    \"\"\"\n",
    "    return dict.keys()\n",
    "\n",
    "\n",
    "CLASSES = {'Agaricus' : 0,'Amanita' : 1,'Boletus' : 2,'Cortinarius' : 3,'Entoloma' : 4,'Hygrocybe' : 5,\n",
    "    'Lactarius' : 6,'Russula' : 7,'Suillus' : 8\n",
    "                }\n",
    "\n",
    "def get_label(key ,class_mush = CLASSES):\n",
    "    \"\"\"\n",
    "    You can get the labeled name of the mushroom by giving \n",
    "    the index or the index by giving the key\n",
    "    \"\"\"\n",
    "    if type(key) == str:\n",
    "        return class_mush[key]\n",
    "    elif type(key) == int:\n",
    "        return list(CLASSES.keys())[list(CLASSES.values()).index(key)]\n",
    "\n",
    "\n",
    "def upload_dir_labels(directory,mapping =CLASSES, k = 300):\n",
    "    \"\"\"\n",
    "    give the list of photos from directory and the labels.\n",
    "    k = number of pictures I want to take from each FOLDER. \n",
    "    \"\"\"\n",
    "    label = []\n",
    "    dir= []\n",
    "    list_of_mushrooms = getList(mapping)\n",
    "    for f in list_of_mushrooms:\n",
    "        pictures_dir = directory + f + '/'\n",
    "        pictures = [pics for pics in listdir(pictures_dir) if isfile(join(pictures_dir , pics))]\n",
    "        for idx, picture in enumerate(pictures[:k]): # take the first 300 pictures from each category\n",
    "            label.append(get_label(f))  \n",
    "            dir.append(pictures_dir + picture) \n",
    "        \n",
    "    return dir, label\n",
    "\n",
    "def image_reshape(dir, batch, new_size = (224,224),k = 300, gray =False):\n",
    "    \"\"\"\n",
    "    give the list of photos with the correct pixel size you want to \n",
    "    downsize. If you want Gray then write True.\n",
    "    \"\"\"\n",
    "    items_resized = []\n",
    "    for idx, dir in enumerate(dir[(batch-1)*k:batch * k]):     \n",
    "        item = cv2.imread(dir)\n",
    "        if gray == True:\n",
    "            item_gray = cv2.cvtColor(item, cv2.COLOR_BGR2GRAY)\n",
    "            item_resized_gray = resize(item_gray, new_size)\n",
    "            items_resized.append(item_resized_gray)\n",
    "        else:\n",
    "            item_resized_colored = resize(item, new_size)\n",
    "            items_resized.append(item_resized_colored)\n",
    "    with open(directory + \"downsized/mushrooms_resized_part_\"+ str(batch), \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(items_resized, fp)\n",
    "    return items_resized\n",
    "\n",
    "\n",
    "def downsize_in_batches(directories):\n",
    "    \"\"\"\n",
    "    This function downsizes the images in batches of 300pics.\n",
    "    directories = list of the directories of the images.\n",
    "    \"\"\"\n",
    "    batch = 1\n",
    "    for idx, dir in enumerate(directories):\n",
    "\n",
    "        if idx % 300 == 0:\n",
    "            print(batch)\n",
    "            image_reshape(directories,batch)\n",
    "            batch += 1 \n",
    "\n",
    "\n",
    "def unpickle(directory , mapping):\n",
    "    \"\"\"\n",
    "    This functions takes the data from a directory that have been\n",
    "    downsized in banches ang gives back the X,y as arrays.\n",
    "    \"\"\"\n",
    "    \n",
    "    table_mushrooms_downsized = []\n",
    "    directories, labels = upload_dir_labels(directory,mapping, k=300)\n",
    "    for batch in range(1,10): # 10 is the number of batches\n",
    "        with open(directory + \"downsized/mushrooms_resized_part_\"+ str(batch), \"rb\") as fp:   # Unpickling\n",
    "            li = pickle.load(fp)\n",
    "            table_mushrooms_downsized.append(li)\n",
    "\n",
    "    # flatten the nested list into a 1D list\n",
    "    mushrooms_downsized = list(itertools.chain(*table_mushrooms_downsized))\n",
    "    if len(mushrooms_downsized) == len(labels):\n",
    "\n",
    "        #Check if the number of pictures and labels are the same.\n",
    "        X = np.asarray(mushrooms_downsized)\n",
    "        y = np.asarray(labels)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def isqrt(n):\n",
    "    \"\"\"\n",
    "    This returns the largest integer x for which x * x does not exceed n. \n",
    "    If you want to check if the result is exactly the square root, simply \n",
    "    perform the multiplication to check if n is a perfect square.\n",
    "    \"\"\"\n",
    "    x = n\n",
    "    y = (x + 1) // 2\n",
    "    while y < x:\n",
    "        x = y\n",
    "        y = (x + n // x) // 2\n",
    "    return x\n",
    "def plot_mushroom_coll(x = 25):\n",
    "    \"\"\"\n",
    "    We can see a collection of random mushrooms with the labels\n",
    "    x is the number of mushrooms you want to see and it will give \n",
    "    the appropriate number of n * n array.\n",
    "    \"\"\"\n",
    "    for i in range(x):\n",
    "        ran = random.randint(0,len(y)-1)\n",
    "        plt.subplot(int(np.sqrt(x)), int(np.sqrt(x)), i+1)\n",
    "        plt.imshow(X[ran], cmap='Greys')\n",
    "        plt.axis('off')\n",
    "        plt.text(0, 0, y[ran]) # displays y-values in each subplot\n",
    "\n",
    "\n",
    "def pickle_(): #!!!!!!!!! Not ready function\n",
    "    \"\"\"\n",
    "    This function pickles the data.\n",
    "    \"\"\"\n",
    "    where ='clean_data/data_pool/train/'\n",
    "    with open('X_train','wb') as f: pickle.dump(X_train, f)\n",
    "    os.replace('train', where+ 'X_train')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_preprocessed_data(X ,y, test_size = 0.1, pp = False):\n",
    "    \"\"\"\n",
    "    This function returns the train and validataion data \n",
    "    preprocessed if necessary: in a flattened 1_D form\n",
    "    with normalized X and y per 255.\n",
    "    Split images (75%/15%/10%) and save to temporary folders\n",
    "    \"\"\"\n",
    "    if pp == True:\n",
    "        x= X.reshape(X.shape[0],-1)\n",
    "        x_norm = x.astype(\"float32\") / 255 # Scale images to the [0, 1] range IF not scaled\n",
    "        # one-hot-encode labels\n",
    "        y_ohe = to_categorical(y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x_norm, y_ohe, test_size = test_size, random_state=42)\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = test_size * 1.7, random_state=42) \n",
    "    else:\n",
    "        x_norm = X.astype(\"float32\") \n",
    "        # one-hot-encode labels\n",
    "        y_ohe = to_categorical(y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x_norm, y_ohe, test_size = test_size, random_state=42)\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = test_size * 1.7, random_state=42)  \n",
    "    # file_name = 'data/Data/Mushrooms/'\n",
    "    \n",
    "    # pickle_(file_name + \"train\", X_train)\n",
    "    # pickle_(file_name + \"train\", y_train)\n",
    "    # pickle_(file_name + \"test\",  X_test)\n",
    "    # pickle_(file_name + \"test\",  y_test)\n",
    "    # pickle_(file_name + \"valid\", X_valid)\n",
    "    # pickle_(file_name + \"valid\", y_valid)  \n",
    "    return X_train, X_test, y_train, y_test, X_valid, y_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of labels and directories\n",
    "# Run to create batches labels etc\n",
    "directory = 'data/Data/Mushrooms/' \n",
    "directories, labels = upload_dir_labels(directory, k=300)\n",
    "downsize_in_batches()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- took 2 min and 36 sec for batches of 300 pictures per categories (300*300pixs)\n",
    "- took 3 min and 12 sec for batches of 300 pictures per categories (224*224pixs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to unpicle data and give back X,y\n",
    "X ,y = unpickle(directory = 'data/Data/Mushrooms/' , mapping =CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[0],cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mushroom_coll(x = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prepare data to go inside the CNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- flatten data and normalize them to be in 0 to 1 intensity depending on the requirements of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, X_valid, y_valid = split_preprocessed_data(X,y, pp = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0],cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model RESNET_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2, decode_predictions, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image # Keras own inbuild image class\n",
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_ResNet50V2(CLASSES = CLASSES,learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    This function returns a ResNet50V2 model with the last\n",
    "    layer removed.\n",
    "    \"\"\"\n",
    "    K.clear_session() # Always clear the session!\n",
    "\n",
    "    base_model = ResNet50V2(\n",
    "        weights='imagenet', \n",
    "        pooling='avg',      # applies global average pooling to the output of the last conv layer (like a flattening)\n",
    "        include_top=False,   # !!!!! we only want to have the base, not the final dense layers \n",
    "        input_shape=(224, 224, 3)  \n",
    "    )\n",
    "    base_model.trainable = False # To freeze the model\n",
    "    # Start building on top of the model\n",
    "    model = keras.Sequential() # defining a new model\n",
    "    model.add(base_model) # adding in the pretrained model without the fully connected layer\n",
    "    model.add(keras.layers.Dense(64, activation='relu')) # adding in additional layers\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(128, activation='relu')) # adding in additional layers\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(len(CLASSES), activation='softmax')) #!!! Final layer with a length of 2, and softmax activation \n",
    "    # have a look at the trainable and non-trainable params statistic\n",
    "    model.summary()\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate),\n",
    "              loss=keras.losses.categorical_crossentropy,\n",
    "              metrics=[keras.metrics.categorical_accuracy])\n",
    "\n",
    "    return model\n",
    "\n",
    "def fit_ResNet50V2(X_train, y_train,epochs = 5, batch_size =500,sav = False):\n",
    "    \"\"\"\n",
    "    This function fits the model on the training data.\n",
    "    \"\"\"\n",
    "    # observe the validation loss and stop when it does not improve after 3 iterations\n",
    "    callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "    history = model.fit(X_train, y_train, \n",
    "              epochs=epochs, \n",
    "              verbose=2,\n",
    "              batch_size=batch_size, \n",
    "              callbacks=[callback],\n",
    "              # use 30% of the data for validation\n",
    "              validation_split=0.3)\n",
    "    # save model\n",
    "    if sav == True:\n",
    "        model.save('models/ResNet50V2_RGB_300fot_SD.h5')\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_ResNet50V2()\n",
    "\n",
    "history = fit_ResNet50V2(X_train, y_train, epochs = 50, batch_size =500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
